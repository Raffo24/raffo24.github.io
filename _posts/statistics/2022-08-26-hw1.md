---
title: Homework 1 - Theory
classes: wide
date: 2024-10-06 01:27:00 +0200
header:
  teaser: /assets/images/blog/statistics/hw1.png
ribbon: MidnightBlue
categories:
  - Statistics
toc: true
---

## What is the Statistics?

Statistics is a branch of mathematics that involves collecting, organizing, analyzing, interpreting, and presenting data.
The goal of statistics is to extract meaningful insights and draw valid conclusions from data.

## Basic Notions in Statistics

- **Population:** in statistics, the population represents the complete set of all elements (units) being studied. The **statistical population** encompasses all measurements of each unit across the entire population for which data is accessible. 
- **Statistical Units:** A **unit** refers to an individual object or person whose attributes are being examined. For each unit, a variety of selected attributes are examined based on the research focus.
- **Distribution:** refers to the way in which data values are spread or arranged across a given range. It provides a summary of how often different values or ranges of values occur in a dataset. The distribution helps identify patterns, trends, and characteristics of the data, and it is crucial for statistical analysis and interpretation.
<br>


## Averages in Statistics
In statistics, we use specific measures to calculate averages, and these are collectively called measures of central tendency. The most common measures of central tendency are:
- **Mean**: The sum of all values divided by the total number of values. It is also called the "**Arithmetic Mean**" or "Arithmetic Average". <br>
 ```python
	 def mean(data):
		 n = len(data)
		 return sum(data)/n
 ```
- **Median**: The middle value when the data is arranged in ascending order.
   *if the data has a even number of values the median is the average of the two middle values* <br>
    ```python
	def median(data):
		n = len(data)
		if n % 2 == 0: # If the data has an even number of values
			median_value = (data[n//2] + data[n//2 + 1]) / 2 
		else: # If the data has an odd number of values
			median_value = data[n//2]
		return median_value
	```
- **Mode**: The value that appears most frequently in the dataset. <br>
```python
	def find_mode(ls):
		dic = {}
		max_count = 0
		mode = None
		for val in ls:
			dic[val] += 1
			if dic[val] > max_count:
				max_count = dic[val]
				mode = val
		return mode
```
**Geometric Mean**: The geometric mean is calculated by taking the product of all values together and then taking the n-th root of that product, where n is the number of values. <br><br>

$$ \left( \prod_{i=1}^{n} x_{i} \right)^{\frac{1} {n}}=\root n \of{x_{1} x_{2} \cdots x_{n}} $$

<br><br>
```python
def geometricMean(ls):
	n = len(ls)
    product=1
    for val in ls:
        product*=val
    return product**(1/n)
```
- **Harmonic Mean**: The harmonic mean is calculated by taking the total number of values in the set and dividing that by the sum of the reciprocals of each value in the set
<br><br>
 
 $$ \frac{n} {\sum_{i=1}^{n} {\frac{1} {x_{i}}}} $$

 <br><br>
```python
def harmonicMean(ls):
	somma = 0
	for val in ls:
	    somma += 1/val   
	return len(test_list)/somma
```
<br>

## Floating Point Representation: Errors and Numerical Stability

For statisticians, recognizing errors in computation is vital for ensuring the accuracy and reliability of their analyses.

Floating-point representation inherently involves approximating real numbers, which leads to unavoidable inaccuracies. These errors arise from the limitation of using a finite number of bits to represent an infinite range of real numbers. 

The concept of **significant figures**, or more accurately, **relative error**, is key to understanding the behavior of floating-point arithmetic. Relative error is defined as:
<br><br>

$$ relativeError = \frac{approximateValue - trueValue}{trueValue} $$

<br><br>
**This measure quantifies the inaccuracy in the representation of a number.** <br>
*In floating-point arithmetic, due to limited precision, small errors can accumulate and potentially grow as more operations are performed*. <br>
Floating-point arithmetic can encounter specific problems:
- **Rounding Errors**: Rounding errors arise from the limited precision in representing real numbers with finite bits, preventing exact representation of certain values like 0.1 in binary format. The decimal number **0.1** cannot be represented exactly in binary because its binary equivalent results in an infinite repeating fraction. 
<br><br> $$0.1_{10} = 0.0001\overline{1001}_2$$ <br> <br>
Due to the limited number of bits used in floating-point representation, this repeating pattern cannot be stored precisely.
- **Propagation of Errors**: In iterative algorithms, minor errors can accumulate significantly, affecting accuracy. Conducting stability analysis is crucial to understand how errors propagate and assess an algorithm's reliability in handling small input variations.
- **Subtraction of nearly equal values**: Subtraction of nearly equal values can greatly amplify the relative error. This phenomenon is known as **catastrophic cancellation**. Consider the subtraction of two nearly equal numbers, such as 
<br><br> 

$$ x  = 99999993.0 - 99999991.0. $$

<br> <br>
If both numbers are rounded in the floating point representation, `x` may end up being `0` or a value very close to `0`, resulting in a significant loss of precision in the final result.
- **Overflow & Underflow**: Floating-point formats have limitations on exponent ranges, causing overflow (when results exceed maximum values) and underflow (when results approach zero). Special values like "Inf" or "NaN" are used to represent overflow, while unnormalized representations may indicate the magnitude of underflow.

An algorithm is considered **numerically stable** if the small errors introduced during computation do not grow to distort the final result. In other words, stable algorithms yield accurate results even when small intermediate errors occur.
<br>In his book series **“The Art of Computer Programming,”** **Donald Knuth** offered thorough discussions on the challenges of numerical computation and methods for reducing errors in Volume 2, Chapter titled **“ARITHMETIC.”**
<br>**Donald Knuth** provides various suggestions for enhancing the numerical stability of algorithms:
- Use **correct rounding** at critical points to reduce errors.
- **Prefer tolerance-based comparisons over strict equality** when dealing with floating-point numbers.
- **Avoid premature rounding** during calculations to prevent error accumulation.
- **Avoid Catastrophic Cancellation:** reformulate expressions to minimize the risk of subtracting nearly equal numbers. For instance, can aid in maintaining precision expressing $$ a - b $$ as

$$(a - b) \cdot \frac{(a + b)}{(a + b)}​$$

- **Exploiting Mathematical Properties:** Knuth emphasizes the importance of using simple mathematical laws to enhance the stability of floating-point operations by exploiting properties like symmetry and regularity. One notable method is **rounding to even**, which helps minimize bias and error accumulation in both binary and decimal systems.
- **Empirical Testing**: Knuth emphasizes the need for empirical testing and validation, which includes **stress testing algorithms with a variety of inputs** to identify weaknesses and comparing results from different methods to ensure accuracy and stability.
- **Maximize precision** during intermediate steps to minimize rounding issues, for example methods like **Kahan Summation Algorithm**, help minimize rounding errors that occur when adding a series of numbers; the loop iterates through each element in the input list, adjusting for any potential loss of precision. <br>
```python
	def kahan_sum(ls): 
		somma = 0.0 
		c = 0.0 # the compensation of lost low-order bits
		n = len(ls)
		for i in range(n):
			y = input[i] - c
			t = somma + y
			c = (t - somma) - y
			somma = t
		return somma
```

Other techniques such as **unnormalized arithmetic** and **interval arithmetic** can help manage errors, but they must be applied with care.
## Cyber Attack Simulation

- n --> numero dei servers
- m --> numero degli hackers.
- p --> The hacker has probability p to penetrate each server

## Online Implementation

[Online Attack Simulation](https://raffo24.github.io/statistics/hw1_code/)

## Main Function
![image-center](/assets/images/blog/statistics/codes/mainFunHw1.png){: .align-center}

This is the core function of the program, which executes the following steps:
1. **Input Validation**: The function performs validation on the input to prevent potential errors and bugs.
2. **Array Population**: It populates the `servers` and `hackers` arrays with sequential numbers.
3. **Dataset Generation**: Using the `getDataset` function (which will be discussed later), it generates the dataset.
4. **Maximum Breach Calculation**: It extracts the highest number of breaches performed by a single hacker from the dataset.
5. **Interval Selection**: Based on a series of conditions, it determines the number of intervals to divide the breaches into.
6. **Interval Window Calculation**: It calculates the window size for each interval based on the total number of breaches and the selected number of intervals.
7. **Dataset Division**: The dataset is then divided into the relevant intervals for histogram plotting and distribution calculation.
8. **Empty Space Removal**: Any leading empty spaces in the histogram are removed to ensure accurate visualization.
9. **Plotting**: Finally, the program generates both the Flat Line Graph and the histogram for visual analysis.

## Second Function

![image-center](/assets/images/blog/statistics/codes/genDatasetHw1.png){: .align-center}

## References

- Knuth, Donald. *The Art of Computer Programming* Volume 2. Addison-Wesley, 1997.
- Tanvir Mustafy, Md. Tauhid Ur Rahman. *Statistics and Data Analysis for Engineers and Scientists*, Springer, 2024.